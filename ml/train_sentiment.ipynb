{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.experimental import enable_halving_search_cv # noqa\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from time import time\n",
    "from os.path import join\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "from typing import Any\n",
    "import torch\n",
    "from torch import nn, tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gc\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_DATA_FOLDER = ['..','datasets']\n",
    "\n",
    "df_sentiment = pd.read_csv(join(*SOURCE_DATA_FOLDER,'sentiment.csv'))\n",
    "\n",
    "y_set = np.array(list(df_sentiment['SCORE']))\n",
    "X_set = df_sentiment.to_numpy()[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_calc(name:str, search:HalvingGridSearchCV, X:Any, y:Any) -> HalvingGridSearchCV:\n",
    "    # Saved-state file\n",
    "    fn = name+'.bin'\n",
    "    try:\n",
    "        # Try opening saverd state.\n",
    "        with open(fn,'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except:\n",
    "        # Search best hyperparams.\n",
    "        warnings.filterwarnings('ignore')\n",
    "        search.fit(X, y)\n",
    "        warnings.filterwarnings('default')\n",
    "        # Save state\n",
    "        with open(fn,'wb') as f:\n",
    "            pickle.dump(search,f)\n",
    "        return search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 21440\n",
      "Train X/y: (16000, 652) (16000,)\n",
      "Test X/y: (5440, 652) (5440,)\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.75 * y_set.shape[0])\n",
    "train_size = train_size - train_size%128\n",
    "print('Rows:',y_set.shape[0])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_set,y_set, train_size=train_size)\n",
    "\n",
    "print('Train X/y:',X_train.shape, y_train.shape)\n",
    "print('Test X/y:',X_test.shape, y_test.shape)\n",
    "\n",
    "METHODS = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name\n",
    "log_reg_name = 'LogisticRegression'\n",
    "# Possible LogisticRegression parameters.\n",
    "params = {\n",
    "    'penalty': ['l1','l2','elasticnet'],\n",
    "    'C': np.logspace(-4,4,10),\n",
    "    'solver': ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "    'max_iter': [100,1000,2500,5000]\n",
    "}\n",
    "# Parameter optimizer.\n",
    "classifier = HalvingGridSearchCV(\n",
    "    LogisticRegression(),\n",
    "    param_grid=params,\n",
    "    cv=5,\n",
    "    random_state=int(time()),\n",
    "    n_jobs=3,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate or load saved.\n",
    "classifier = load_or_calc(log_reg_name, classifier, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator: LogisticRegression(C=2.782559402207126, max_iter=5000, solver='newton-cholesky')\n",
      "Accuracy: 0.8216911764705882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\scoop\\apps\\python\\current\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print('Estimator:', classifier.best_estimator_)\n",
    "predicted = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predicted)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "METHODS[log_reg_name] = accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name\n",
    "lsvc_name = 'LinearSVC'\n",
    "# Possible ElasticNet parameters.\n",
    "params = {\n",
    "    'penalty': ['l1','l2'],\n",
    "    'loss': ['hinge', 'squared_hinge'],\n",
    "    'C': np.arange(0.1,50,10),\n",
    "    'max_iter': [100,1000,2500,5000]\n",
    "}\n",
    "# Parameter optimizer.\n",
    "classifier = HalvingGridSearchCV(\n",
    "    LinearSVC(),\n",
    "    param_grid=params,\n",
    "    cv=5,\n",
    "    random_state=int(time()),\n",
    "    n_jobs=3,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate or load saved.\n",
    "classifier = load_or_calc(lsvc_name, classifier, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator: LinearSVC(C=10.1, max_iter=5000)\n",
      "Accuracy: 0.4158088235294118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\scoop\\apps\\python\\current\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print('Estimator:', classifier.best_estimator_)\n",
    "predicted = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predicted)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "METHODS[lsvc_name] = accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test neural net Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name\n",
    "nn_name = 'SentimentNN'\n",
    "\n",
    "class SentimentNN(nn.Module):\n",
    "    def __init__(self, num_features:int=None) -> None:\n",
    "        super().__init__()\n",
    "        if num_features:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(num_features, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256,128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128,1)\n",
    "            )\n",
    "    def forward(self,x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class SentimentData(Dataset):\n",
    "    def __init__(self, X, y) -> None:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __getitem__(self, index) -> Any:\n",
    "        return (\n",
    "            torch.tensor(self.X[index], dtype=torch.float32, device=device), \n",
    "            torch.tensor(self.y[index], dtype=torch.float32, device=device)\n",
    "        )\n",
    "    def __len__(self):\n",
    "      return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoc: 0\n",
      "Loss: tensor(0.3122, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 1\n",
      "Loss: tensor(1.1507, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 2\n",
      "Loss: tensor(0.6971, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 3\n",
      "Loss: tensor(0.5523, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 4\n",
      "Loss: tensor(0.3456, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 5\n",
      "Loss: tensor(0.2045, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 6\n",
      "Loss: tensor(0.1380, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 7\n",
      "Loss: tensor(0.1655, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 8\n",
      "Loss: tensor(0.2978, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 9\n",
      "Loss: tensor(0.1478, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 10\n",
      "Loss: tensor(0.0515, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 11\n",
      "Loss: tensor(0.0509, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 12\n",
      "Loss: tensor(0.0419, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 13\n",
      "Loss: tensor(0.0301, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 14\n",
      "Loss: tensor(0.0306, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 15\n",
      "Loss: tensor(0.0450, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 16\n",
      "Loss: tensor(0.0343, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 17\n",
      "Loss: tensor(0.0241, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 18\n",
      "Loss: tensor(0.0222, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 19\n",
      "Loss: tensor(0.0086, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 20\n",
      "Loss: tensor(0.0256, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 21\n",
      "Loss: tensor(0.0404, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 22\n",
      "Loss: tensor(0.0342, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 23\n",
      "Loss: tensor(0.0290, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 24\n",
      "Loss: tensor(0.0126, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 25\n",
      "Loss: tensor(0.0064, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 26\n",
      "Loss: tensor(0.0143, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 27\n",
      "Loss: tensor(0.0206, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 28\n",
      "Loss: tensor(0.0185, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 29\n",
      "Loss: tensor(0.0122, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 30\n",
      "Loss: tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 31\n",
      "Loss: tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 32\n",
      "Loss: tensor(0.0140, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 33\n",
      "Loss: tensor(0.0089, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 34\n",
      "Loss: tensor(0.0113, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 35\n",
      "Loss: tensor(0.0243, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 36\n",
      "Loss: tensor(0.0202, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 37\n",
      "Loss: tensor(0.0151, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 38\n",
      "Loss: tensor(0.0065, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 39\n",
      "Loss: tensor(0.0059, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 40\n",
      "Loss: tensor(0.0130, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 41\n",
      "Loss: tensor(0.0169, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 42\n",
      "Loss: tensor(0.0179, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 43\n",
      "Loss: tensor(0.0143, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 44\n",
      "Loss: tensor(0.0100, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 45\n",
      "Loss: tensor(0.0062, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 46\n",
      "Loss: tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 47\n",
      "Loss: tensor(0.0067, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 48\n",
      "Loss: tensor(0.0121, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 49\n",
      "Loss: tensor(0.0138, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 50\n",
      "Loss: tensor(0.0119, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 51\n",
      "Loss: tensor(0.0083, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 52\n",
      "Loss: tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 53\n",
      "Loss: tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 54\n",
      "Loss: tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 55\n",
      "Loss: tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 56\n",
      "Loss: tensor(0.0105, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 57\n",
      "Loss: tensor(0.0099, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 58\n",
      "Loss: tensor(0.0090, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 59\n",
      "Loss: tensor(0.0065, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 60\n",
      "Loss: tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 61\n",
      "Loss: tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 62\n",
      "Loss: tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 63\n",
      "Loss: tensor(0.0067, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 64\n",
      "Loss: tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 65\n",
      "Loss: tensor(0.0090, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 66\n",
      "Loss: tensor(0.0105, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 67\n",
      "Loss: tensor(0.0090, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 68\n",
      "Loss: tensor(0.0075, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 69\n",
      "Loss: tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 70\n",
      "Loss: tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 71\n",
      "Loss: tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 72\n",
      "Loss: tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 73\n",
      "Loss: tensor(0.0066, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 74\n",
      "Loss: tensor(0.0109, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 75\n",
      "Loss: tensor(0.0096, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 76\n",
      "Loss: tensor(0.0082, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 77\n",
      "Loss: tensor(0.0064, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 78\n",
      "Loss: tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 79\n",
      "Loss: tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 80\n",
      "Loss: tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 81\n",
      "Loss: tensor(0.0078, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 82\n",
      "Loss: tensor(0.0084, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 83\n",
      "Loss: tensor(0.0098, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 84\n",
      "Loss: tensor(0.0081, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 85\n",
      "Loss: tensor(0.0066, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 86\n",
      "Loss: tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 87\n",
      "Loss: tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 88\n",
      "Loss: tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 89\n",
      "Loss: tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 90\n",
      "Loss: tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 91\n",
      "Loss: tensor(0.0091, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 92\n",
      "Loss: tensor(0.0078, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 93\n",
      "Loss: tensor(0.0062, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 94\n",
      "Loss: tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 95\n",
      "Loss: tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 96\n",
      "Loss: tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 97\n",
      "Loss: tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 98\n",
      "Loss: tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 99\n",
      "Loss: tensor(0.0072, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 100\n",
      "Loss: tensor(0.0068, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 101\n",
      "Loss: tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 102\n",
      "Loss: tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 103\n",
      "Loss: tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 104\n",
      "Loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 105\n",
      "Loss: tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 106\n",
      "Loss: tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 107\n",
      "Loss: tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 108\n",
      "Loss: tensor(0.0065, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 109\n",
      "Loss: tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 110\n",
      "Loss: tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 111\n",
      "Loss: tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 112\n",
      "Loss: tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 113\n",
      "Loss: tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 114\n",
      "Loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 115\n",
      "Loss: tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 116\n",
      "Loss: tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 117\n",
      "Loss: tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 118\n",
      "Loss: tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 119\n",
      "Loss: tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 120\n",
      "Loss: tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 121\n",
      "Loss: tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 122\n",
      "Loss: tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 123\n",
      "Loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 124\n",
      "Loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 125\n",
      "Loss: tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 126\n",
      "Loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 127\n",
      "Loss: tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 128\n",
      "Loss: tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 129\n",
      "Loss: tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 130\n",
      "Loss: tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 131\n",
      "Loss: tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 132\n",
      "Loss: tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 133\n",
      "Loss: tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 134\n",
      "Loss: tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 135\n",
      "Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 136\n",
      "Loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 137\n",
      "Loss: tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 138\n",
      "Loss: tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 139\n",
      "Loss: tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 140\n",
      "Loss: tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 141\n",
      "Loss: tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 142\n",
      "Loss: tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 143\n",
      "Loss: tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 144\n",
      "Loss: tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 145\n",
      "Loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 146\n",
      "Loss: tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 147\n",
      "Loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 148\n",
      "Loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoc: 149\n",
      "Loss: tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor([[-0.9602],\n",
      "        [ 1.0111],\n",
      "        [-0.9557],\n",
      "        ...,\n",
      "        [ 0.0050],\n",
      "        [ 0.9838],\n",
      "        [ 0.9563]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model = SentimentNN()\n",
    "    model.load_state_dict(torch.load(\"SentimentNN.bin\"))\n",
    "except:\n",
    "    sentiment_model = None\n",
    "\n",
    "if not sentiment_model:\n",
    "    # Data batching\n",
    "    data_set = SentimentData(X_train, y_train)\n",
    "    loader = DataLoader(data_set, batch_size=768, shuffle=True)\n",
    "\n",
    "    sentiment_model = SentimentNN(X_train.shape[1]).to(device=device, dtype=torch.float32)\n",
    "    optimizer = torch.optim.Adam(sentiment_model.parameters())\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    epochs = 150\n",
    "    for epoc in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        for X_,y_ in loader:\n",
    "            predicted = sentiment_model(X_)\n",
    "            predicted = predicted.reshape(y_.shape[0])\n",
    "            loss = criterion(predicted, y_)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "        print('Epoc:',epoc)\n",
    "        print('Loss:',loss)\n",
    "    torch.save(sentiment_model.state_dict(), 'SentimentNN.bin')\n",
    "\n",
    "predicted = sentiment_model(tensor(X_test, device=device, dtype=torch.float32))\n",
    "print(predicted)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
