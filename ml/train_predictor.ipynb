{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "Train predictor to predict the next day stock price by giving it N previous days of stock prices in combination with text sentiments for stock analysis posts for each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from time import time\n",
    "from typing import Any\n",
    "import pickle\n",
    "import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch import nn, tensor\n",
    "\n",
    "# Set device for torch.\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_DATA_FOLDER = ['..', 'datasets']\n",
    "# Stock data\n",
    "df_stock_data = pd.read_csv(join(*SOURCE_DATA_FOLDER,'stock_data.csv'))\n",
    "# Analysis posts as features\n",
    "df_news_features = pd.read_csv(join(*SOURCE_DATA_FOLDER,'news_features.csv'))\n",
    "# Sentiment classifier model\n",
    "with open('LogisticRegression.bin', 'rb') as f:\n",
    "    sentiment_model,_ = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2020-01-01 2020-03-31\n"
     ]
    }
   ],
   "source": [
    "# Create list of stocks where each stock is a map of dates to values\n",
    "stock_sets = []\n",
    "for label, df_shock in df_stock_data.groupby(by='Label'):\n",
    "    stock = {}\n",
    "    for idx, row in df_shock.iterrows():\n",
    "        stock[row['Date']] = row['Value']\n",
    "    stock_sets.append(stock)\n",
    "# Get min and max dates (use last stock handled)\n",
    "dates = list(stock.keys())\n",
    "dates.sort()\n",
    "first_date = dt.date.fromisoformat(dates[0])\n",
    "last_date = dt.date.fromisoformat(dates[-1])\n",
    "# Deltas\n",
    "one_day = dt.timedelta(1)\n",
    "print('Date range:', first_date, last_date)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert news post features to sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2020-01-01': 1.0, '2020-01-02': 1.0, '2020-01-03': 1.0, '2020-01-04': 0.0, '2020-01-05': 1.0, '2020-01-06': -1.0, '2020-01-07': -1.0, '2020-01-08': -1.0, '2020-01-09': 1.0, '2020-01-10': 1.0, '2020-01-11': -1.0, '2020-01-12': 1.0, '2020-01-13': 1.0, '2020-01-14': 1.0, '2020-01-15': 1.0, '2020-01-16': 1.0, '2020-01-17': 1.0, '2020-01-18': 0.0, '2020-01-19': 1.0, '2020-01-20': 1.0, '2020-01-21': 1.0, '2020-01-22': 1.0, '2020-01-23': 1.0, '2020-01-24': 1.0, '2020-01-25': -1.0, '2020-01-26': 1.0, '2020-01-27': -1.0, '2020-01-28': 1.0, '2020-01-29': 1.0, '2020-01-30': 1.0, '2020-01-31': 1.0, '2020-02-01': 1.0, '2020-02-02': -1.0, '2020-02-03': 1.0, '2020-02-04': 1.0, '2020-02-05': 1.0, '2020-02-06': 1.0, '2020-02-07': -1.0, '2020-02-08': 1.0, '2020-02-09': 1.0, '2020-02-10': 1.0, '2020-02-11': 1.0, '2020-02-12': 1.0, '2020-02-13': 1.0, '2020-02-14': 1.0, '2020-02-15': 1.0, '2020-02-16': 1.0, '2020-02-17': -1.0, '2020-02-18': 1.0, '2020-02-19': 1.0, '2020-02-20': 1.0, '2020-02-21': 1.0, '2020-02-22': 1.0, '2020-02-23': 1.0, '2020-02-24': -1.0, '2020-02-25': -1.0, '2020-02-26': 1.0, '2020-02-27': 1.0, '2020-02-28': -1.0, '2020-02-29': 1.0, '2020-03-01': 1.0, '2020-03-02': -1.0, '2020-03-03': -1.0, '2020-03-04': 1.0, '2020-03-05': -1.0, '2020-03-06': -1.0, '2020-03-07': 1.0, '2020-03-08': -1.0, '2020-03-09': -1.0, '2020-03-10': -1.0, '2020-03-11': -1.0, '2020-03-12': -1.0, '2020-03-13': -1.0, '2020-03-14': 1.0, '2020-03-15': 1.0, '2020-03-16': -1.0, '2020-03-17': -1.0, '2020-03-18': -1.0, '2020-03-19': -1.0, '2020-03-20': 1.0, '2020-03-21': 1.0, '2020-03-22': 1.0, '2020-03-23': -1.0, '2020-03-24': 1.0, '2020-03-25': 1.0, '2020-03-26': -1.0, '2020-03-27': -1.0, '2020-03-28': 1.0, '2020-03-29': 1.0, '2020-03-30': -1.0, '2020-03-31': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Create map of sentiments\n",
    "posts = {}\n",
    "# dates\n",
    "post_dates = list(df_news_features['DATE'])\n",
    "# features\n",
    "news_features = df_news_features.drop('DATE', axis=1).to_numpy()[:, 1:]\n",
    "# predictions\n",
    "sentiment_predicted = sentiment_model.predict(news_features)\n",
    "# build dictionary of predictions\n",
    "for i in range(len(post_dates)):\n",
    "    posts[post_dates[i]] = sentiment_predicted[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 3\n",
      "Start: 2020-01-04\n",
      "Samples per stock: 87\n",
      "Sample set count: 1653\n",
      "Row 0: [0.42 0.42 0.42 1.   1.   1.  ] 0.42\n"
     ]
    }
   ],
   "source": [
    "# Previous days\n",
    "N = 3\n",
    "# Prediction start date\n",
    "start_date = first_date + one_day * N\n",
    "# sample count per stock\n",
    "samples = (last_date - start_date).days\n",
    "print('N:',N)\n",
    "print('Start:',start_date)\n",
    "print('Samples per stock:',samples)\n",
    "X_data=[]\n",
    "y_data=[]\n",
    "# generate from all stocks\n",
    "for stock in stock_sets:\n",
    "    date = start_date\n",
    "    # do all valid prediction dates\n",
    "    for offset in range(samples):\n",
    "        prediction_date = start_date + one_day * offset\n",
    "        # array holding stock values in first N spaces\n",
    "        # and sentiments in last N\n",
    "        X = np.zeros(N*2)\n",
    "        train_date_start = prediction_date - one_day*N\n",
    "        for i in range(0,N):\n",
    "            date_ = (train_date_start + one_day*i).isoformat()\n",
    "            # stock\n",
    "            X[i] = stock[date_]\n",
    "            # sentiment\n",
    "            X[N+i] = posts[date_]\n",
    "        y_data.append(stock[prediction_date.isoformat()])\n",
    "        X_data.append(X)\n",
    "y_data = np.array(y_data, dtype=np.float32)\n",
    "X_data = np.array(X_data, dtype=np.float32)\n",
    "print('Sample set count:',len(X_data))\n",
    "print('Row 0:',X_data[0],y_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name\n",
    "nn_name = 'PredictorNN'\n",
    "\n",
    "class PredictorNN(nn.Module):\n",
    "    \"\"\"Input is 3 days of sentiment and stock values.\n",
    "    Output is next day stock value.\n",
    "    \"\"\"\n",
    "    INPUT = 6\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.INPUT, self.INPUT*10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.INPUT*7, self.INPUT*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.INPUT*4, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "    def save(self, path: str) -> bool:\n",
    "        try:\n",
    "            torch.save(self.state_dict(), path)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def load(self, path: str) -> bool:\n",
    "        try:\n",
    "            self.load_state_dict(torch.load(path))\n",
    "            self.eval()\n",
    "            return True\n",
    "        except:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss=2849.645751953125\n",
      "Epoch 1000: loss=2.930873155593872\n",
      "Epoch 2000: loss=2.8580358028411865\n",
      "Epoch 3000: loss=2.8031725883483887\n",
      "Epoch 4000: loss=2.767005681991577\n",
      "Epoch 5000: loss=2.7020082473754883\n",
      "Epoch 6000: loss=2.615553855895996\n",
      "Epoch 7000: loss=2.57550048828125\n",
      "Epoch 8000: loss=2.551692247390747\n",
      "Epoch 9000: loss=2.532294750213623\n",
      "Epoch 10000: loss=2.5272183418273926\n",
      "Epoch 11000: loss=2.508028268814087\n",
      "Epoch 12000: loss=2.491166591644287\n",
      "Epoch 13000: loss=2.4852395057678223\n",
      "Epoch 14000: loss=2.4796905517578125\n",
      "Epoch 15000: loss=2.4695403575897217\n",
      "Epoch 16000: loss=2.46474289894104\n",
      "Epoch 17000: loss=2.4591360092163086\n",
      "Epoch 18000: loss=2.459909677505493\n",
      "Epoch 19000: loss=2.4529013633728027\n",
      "Epoch 20000: loss=2.4354171752929688\n",
      "Epoch 21000: loss=2.409074544906616\n",
      "Epoch 22000: loss=2.4051096439361572\n",
      "Epoch 23000: loss=2.396021842956543\n",
      "Epoch 24000: loss=2.391101121902466\n",
      "Epoch 25000: loss=2.3813202381134033\n",
      "Epoch 26000: loss=2.3784120082855225\n",
      "Epoch 27000: loss=2.3748178482055664\n",
      "Epoch 28000: loss=2.384648561477661\n",
      "Epoch 29000: loss=2.369497060775757\n",
      "Epoch 30000: loss=2.367882013320923\n",
      "Epoch 31000: loss=2.3740408420562744\n",
      "Epoch 32000: loss=2.366887331008911\n",
      "Epoch 33000: loss=2.3645217418670654\n",
      "Epoch 34000: loss=2.3639144897460938\n",
      "Epoch 35000: loss=2.3794901371002197\n",
      "Epoch 36000: loss=2.3611762523651123\n",
      "Epoch 37000: loss=2.3593859672546387\n",
      "Epoch 38000: loss=2.3624513149261475\n",
      "Epoch 39000: loss=2.361255168914795\n",
      "Epoch 40000: loss=2.3554985523223877\n",
      "Epoch 41000: loss=2.3588597774505615\n",
      "Epoch 42000: loss=2.354294538497925\n",
      "Epoch 43000: loss=2.366546154022217\n",
      "Epoch 44000: loss=2.3573672771453857\n",
      "Epoch 45000: loss=2.3709959983825684\n",
      "Epoch 46000: loss=2.3545501232147217\n",
      "Epoch 47000: loss=2.3730549812316895\n",
      "Epoch 48000: loss=2.35886812210083\n",
      "Epoch 49000: loss=2.3528757095336914\n",
      "Epoch 50000: loss=2.3537330627441406\n",
      "Epoch 51000: loss=2.3510262966156006\n",
      "Epoch 52000: loss=2.350492238998413\n",
      "Epoch 53000: loss=2.350452184677124\n",
      "Epoch 54000: loss=2.3485889434814453\n",
      "Epoch 55000: loss=2.3690595626831055\n",
      "Epoch 56000: loss=2.3476626873016357\n",
      "Epoch 57000: loss=2.349778890609741\n",
      "Epoch 58000: loss=2.3484790325164795\n",
      "Epoch 59000: loss=2.3488409519195557\n",
      "Epoch 60000: loss=2.3461246490478516\n",
      "Epoch 61000: loss=2.345475912094116\n",
      "Epoch 62000: loss=2.347744941711426\n",
      "Epoch 63000: loss=2.3438806533813477\n",
      "Epoch 64000: loss=2.3498873710632324\n",
      "Epoch 65000: loss=2.345519542694092\n",
      "Epoch 66000: loss=2.341644287109375\n",
      "Epoch 67000: loss=2.351148843765259\n",
      "Epoch 68000: loss=2.3405187129974365\n",
      "Epoch 69000: loss=2.3408782482147217\n",
      "Epoch 70000: loss=2.350482702255249\n",
      "Epoch 71000: loss=2.3484294414520264\n",
      "Epoch 72000: loss=2.3436572551727295\n",
      "Epoch 73000: loss=2.339911937713623\n",
      "Epoch 74000: loss=2.346050500869751\n",
      "Epoch 75000: loss=2.3425614833831787\n",
      "Epoch 76000: loss=2.3396167755126953\n",
      "Epoch 77000: loss=2.3383820056915283\n",
      "Epoch 78000: loss=2.3381426334381104\n",
      "Epoch 79000: loss=2.3381974697113037\n",
      "Epoch 80000: loss=2.348442554473877\n",
      "Epoch 81000: loss=2.336867570877075\n",
      "Epoch 82000: loss=2.3351433277130127\n",
      "Epoch 83000: loss=2.343345880508423\n",
      "Epoch 84000: loss=2.3403751850128174\n",
      "Epoch 85000: loss=2.3674700260162354\n",
      "Epoch 86000: loss=2.3606321811676025\n",
      "Epoch 87000: loss=2.332704544067383\n",
      "Epoch 88000: loss=2.332535982131958\n",
      "Epoch 89000: loss=2.3415870666503906\n",
      "Epoch 90000: loss=2.33227801322937\n",
      "Epoch 91000: loss=2.3318002223968506\n",
      "Epoch 92000: loss=2.337963104248047\n",
      "Epoch 93000: loss=2.338106870651245\n",
      "Epoch 94000: loss=2.3306005001068115\n",
      "Epoch 95000: loss=2.3325459957122803\n",
      "Epoch 96000: loss=2.332698106765747\n",
      "Epoch 97000: loss=2.35270094871521\n",
      "Epoch 98000: loss=2.3305766582489014\n",
      "Epoch 99000: loss=2.3438236713409424\n",
      "Last epoch 99999: loss=2.3331680297851562, time=160.5058467388153\n"
     ]
    }
   ],
   "source": [
    "model_file = f'{nn_name}.bin'\n",
    "predictor_model = PredictorNN().to(\n",
    "    device=device, dtype=torch.float32)\n",
    "\n",
    "if not predictor_model.load(model_file):\n",
    "    # create optimizer cunction\n",
    "    optimizer = torch.optim.Adam(predictor_model.parameters())\n",
    "    # create loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    # set passes\n",
    "    passes = 100000\n",
    "    start_time = time()\n",
    "\n",
    "    X_ = tensor(X_data, dtype=torch.float32, device=device)\n",
    "    y_ = tensor(y_data, dtype=torch.float32, device=device)\n",
    "\n",
    "    # do training iterations\n",
    "    for epoch in range(passes):\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        # batch load data each pass\n",
    "        # prediction\n",
    "        predicted = predictor_model(X_).reshape(y_.shape)\n",
    "        # calculate cost\n",
    "        loss = criterion(predicted, y_)\n",
    "        # calculate gradients\n",
    "        loss.backward()\n",
    "        # update nn weights\n",
    "        optimizer.step()\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f'Epoch {epoch}: loss={loss}')\n",
    "    print(f'Last epoch {epoch}: loss={loss}, time={time()-start_time}')\n",
    "    # save results to file\n",
    "    predictor_model.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test accuracy\n",
    "print('Accuracy: ??')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7df6328e4bf7031429de322cc3979dd311d6be76efa5b8a05f8fe712b91c63c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
