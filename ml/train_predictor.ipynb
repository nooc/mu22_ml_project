{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "Train predictor to predict the next day stock price by giving it N previous days of stock price changes in combination with text sentiments for stock analysis posts for each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from time import time\n",
    "from typing import Any\n",
    "import pickle\n",
    "import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import get_scorer\n",
    "\n",
    "from torch import nn, tensor\n",
    "\n",
    "# Set device for torch.\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_DATA_FOLDER = ['..', 'datasets']\n",
    "# Stock data\n",
    "df_stock_data = pd.read_csv(join(*SOURCE_DATA_FOLDER,'stock_data.csv'))\n",
    "# Analysis posts as features\n",
    "df_news_features = pd.read_csv(join(*SOURCE_DATA_FOLDER,'news_features.csv'))\n",
    "# Sentiment classifier model\n",
    "with open('LogisticRegression.bin', 'rb') as f:\n",
    "    sentiment_model,_ = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2020-01-01 2020-03-31\n"
     ]
    }
   ],
   "source": [
    "# Create list of stocks where each stock is a map of dates to values\n",
    "stock_sets = []\n",
    "for label, df_shock in df_stock_data.groupby(by='Label'):\n",
    "    stock = {}\n",
    "    for idx, row in df_shock.iterrows():\n",
    "        stock[row['Date']] = row['Value']\n",
    "    stock_sets.append(stock)\n",
    "# Get min and max dates (use last stock handled)\n",
    "dates = list(stock.keys())\n",
    "dates.sort()\n",
    "first_date = dt.date.fromisoformat(dates[0])\n",
    "last_date = dt.date.fromisoformat(dates[-1])\n",
    "# Deltas\n",
    "one_day = dt.timedelta(1)\n",
    "print('Date range:', first_date, last_date)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert news post features to sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create map of sentiments\n",
    "posts = {}\n",
    "# dates\n",
    "post_dates = list(df_news_features['DATE'])\n",
    "# features\n",
    "news_features = df_news_features.drop('DATE', axis=1).to_numpy()[:, 1:]\n",
    "# predictions\n",
    "sentiment_predicted = sentiment_model.predict(news_features)\n",
    "# build dictionary of predictions\n",
    "for i in range(len(post_dates)):\n",
    "    posts[post_dates[i]] = sentiment_predicted[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 3\n",
      "Start: 2020-01-04\n",
      "Samples per stock: 87\n",
      "Sample set count: 1653\n",
      "Train count: 1322\n",
      "Test count: 331\n",
      "[[ 1.          0.9808467   0.99596775 -1.         -1.          1.        ]\n",
      " [ 1.          1.          1.          1.         -1.         -1.        ]\n",
      " [ 1.          0.9725628   0.95125514  1.         -1.         -1.        ]\n",
      " [ 1.          0.9928058   0.9223022  -1.          1.         -1.        ]\n",
      " [ 1.          1.          1.          1.          1.          1.        ]]\n",
      "[0.96874994 1.         0.9454174  0.88201445 1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Previous days\n",
    "N = 3\n",
    "# Prediction start date\n",
    "start_date = first_date + one_day * N\n",
    "# sample count per stock\n",
    "samples = (last_date - start_date).days\n",
    "print('N:',N)\n",
    "print('Start:',start_date)\n",
    "print('Samples per stock:',samples)\n",
    "X_data=[]\n",
    "y_data=[]\n",
    "# generate from all stocks\n",
    "for stock in stock_sets:\n",
    "    date = start_date\n",
    "    # do all valid prediction dates\n",
    "    for offset in range(samples):\n",
    "        prediction_date = start_date + one_day * offset\n",
    "        # array holding stock values in first N spaces\n",
    "        # and sentiments in last N\n",
    "        X = np.zeros(N*2)\n",
    "        train_date_start = prediction_date - one_day*N\n",
    "        for i in range(0,N):\n",
    "            date_ = (train_date_start + one_day*i).isoformat()\n",
    "            # Get stock change (day 0 == 1.0).\n",
    "            if i==0:\n",
    "                X[i] = 1.0\n",
    "                base = stock[date_]\n",
    "            else: X[i] = stock[date_] / base\n",
    "            # sentiment\n",
    "            X[N+i] = posts[date_]\n",
    "        y_data.append(stock[prediction_date.isoformat()]/base)\n",
    "        X_data.append(X)\n",
    "y_data = np.array(y_data, dtype=np.float32)\n",
    "X_data = np.array(X_data, dtype=np.float32)\n",
    "print('Sample set count:',len(X_data))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2)\n",
    "print('Train count:',len(X_train))\n",
    "print('Test count:',len(X_test))\n",
    "print(X_train[:5])\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name\n",
    "nn_name = 'PredictorNN'\n",
    "\n",
    "class PredictorNN(nn.Module):\n",
    "    \"\"\"Input is 3 days of sentiment and stock values.\n",
    "    Output is next day stock value.\n",
    "    \"\"\"\n",
    "    INPUT = 6\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.INPUT, self.INPUT*10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.INPUT*10, self.INPUT*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.INPUT*4, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "    def save(self, path: str) -> bool:\n",
    "        try:\n",
    "            torch.save(self.state_dict(), path)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def load(self, path: str) -> bool:\n",
    "        try:\n",
    "            self.load_state_dict(torch.load(path))\n",
    "            self.eval()\n",
    "            return True\n",
    "        except:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = f'{nn_name}.bin'\n",
    "predictor_model = PredictorNN().to(\n",
    "    device=device, dtype=torch.float32)\n",
    "\n",
    "if not predictor_model.load(model_file):\n",
    "    # create optimizer cunction\n",
    "    optimizer = torch.optim.Adam(predictor_model.parameters())\n",
    "    # create loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    # set passes\n",
    "    passes = 100000\n",
    "    start_time = time()\n",
    "\n",
    "    X_ = tensor(X_train, dtype=torch.float32, device=device)\n",
    "    y_ = tensor(y_train, dtype=torch.float32, device=device)\n",
    "\n",
    "    # do training iterations\n",
    "    for epoch in range(passes):\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        # batch load data each pass\n",
    "        # prediction\n",
    "        predicted = predictor_model(X_).reshape(y_.shape)\n",
    "        # calculate cost\n",
    "        loss = criterion(predicted, y_)\n",
    "        # calculate gradients\n",
    "        loss.backward()\n",
    "        # update nn weights\n",
    "        optimizer.step()\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f'Epoch {epoch}: loss={loss}')\n",
    "    print(f'Last epoch {epoch}: loss={loss}, time={time()-start_time}')\n",
    "    # save results to file\n",
    "    predictor_model.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9775905407447469\n",
      "X test: tensor([[ 1.0000,  1.0952,  1.0105, -1.0000, -1.0000, -1.0000],\n",
      "        [ 1.0000,  1.0366,  1.0610,  1.0000, -1.0000, -1.0000],\n",
      "        [ 1.0000,  0.9764,  0.9941, -1.0000, -1.0000,  1.0000],\n",
      "        [ 1.0000,  0.9839,  0.9471, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0885,  1.0885, -1.0000, -1.0000,  1.0000]],\n",
      "       device='cuda:0')\n",
      "  Pred: [0.9677838  1.0771403  0.98829985 0.9388577  1.0797765 ]\n",
      "  Test: [1.1162646 1.097561  0.9587195 0.9280191 1.0885341]\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy\n",
    "from sklearn.metrics import r2_score\n",
    "scorer = get_scorer('r2')\n",
    "X_ = tensor(X_test, dtype=torch.float32, device=device)\n",
    "y_pred = predictor_model(X_).cpu().detach().numpy().reshape(y_test.shape)\n",
    "accuracy = 1.0 - np.absolute(y_pred - y_test).sum()/y_pred.shape[0]\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('X test:', X_[:5])\n",
    "print('  Pred:', y_pred[:5])\n",
    "print('  Test:', y_test[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7df6328e4bf7031429de322cc3979dd311d6be76efa5b8a05f8fe712b91c63c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
